{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pythainlp\n",
    "# !pip3 install https://github.com/PyThaiNLP/thai_sentiment_analysis/archive/master.zip\n",
    "# !pip3 install kenlm==0.2.0\n",
    "# !pip3 install pypdf==3.17.1\n",
    "# !pip3 install pytesseract==0.3.10\n",
    "# !pip3 install PyMuPDF==1.23.6\n",
    "# !pip3 install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer\n",
    "\n",
    "text = \"สมชายเห็นชอบกลบทบาทนี้\"\n",
    "\n",
    "print(\"newmm  :\", word_tokenize(text))\n",
    "print(\"longest:\", word_tokenize(text, engine=\"longest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = {\n",
    "    \"Word\": [\"แล\", \"เบิ่ง\", \"ผ่อ\"],\n",
    "    \"POS\" : [\"ก.\", \"ก.\", \"ก.\"],\n",
    "    \"Definition\": [\"ดู มอง\", \"ดู มอง เหลียวดู\", \"ดู ดูแล มอง\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_cosine_similarity(definitions):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(definitions)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def pos_similarity(pos_list):\n",
    "    n = len(pos_list)\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += 1 if pos_list[i] == pos_list[j] else 0\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "def definition_similarity(definitions):\n",
    "    similarity_matrix = calculate_cosine_similarity(definitions)\n",
    "    n = similarity_matrix.shape[0]\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += similarity_matrix[i, j]\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "pos = df[\"POS\"].values\n",
    "pos_sim = pos_similarity(pos)\n",
    "\n",
    "definitions = df[\"Definition\"]\n",
    "def_sim = definition_similarity(definitions)\n",
    "similarity = alpha * pos_sim + beta * def_sim\n",
    "\n",
    "print(f\"POS Similarity: {pos_sim:.2f}\")\n",
    "print(f\"Definition Similarity: {def_sim:.2f}\")\n",
    "print(f\"Overall Similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class TFIDFCalculator:\n",
    "    def __init__(self, documents: List[List[str]]):\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        self.term_freq = [Counter(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "    def calculate_tf(self, term: str, doc_idx: int) -> float:\n",
    "        if self.doc_lengths[doc_idx] == 0:\n",
    "            return 0\n",
    "        return self.term_freq[doc_idx][term] / self.doc_lengths[doc_idx]\n",
    "\n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        doc_with_term = sum(1 for doc in self.documents if term in doc)\n",
    "        if doc_with_term == 0:\n",
    "            return 0\n",
    "        return math.log2(self.doc_count / doc_with_term)\n",
    "\n",
    "    def calculate_tfidf(self, terms: List[str]) -> Dict[str, List[float]]:\n",
    "        results = {}\n",
    "        for term in terms:\n",
    "            idf = self.calculate_idf(term)\n",
    "            tfidf_scores = [\n",
    "                round(self.calculate_tf(term, doc_idx) * idf, 4)\n",
    "                for doc_idx in range(self.doc_count)\n",
    "            ]\n",
    "            results[term] = tfidf_scores\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    documents = [\n",
    "        [\"นวัตกรรม\", \"พลังงาน\", \"สะอาด\", \"เพื่อ\", \"โลก\",\n",
    "        \"ยั่งยืน\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\", \"ลม\",\n",
    "        \"กำลัง\", \"เป็นที่นิยม\", \"ใน\", \"ประเทศไทย\", \"นักวิทยาศาสตร์\",\n",
    "        \"คาดว่า\", \"จะ\", \"ช่วย\", \"ลด\", \"การปล่อย\",\n",
    "        \"ก๊าซ\", \"เรือนกระจก\", \"ได้\", \"อย่างมาก\"],\n",
    "        [\"เศรษฐกิจ\", \"ไทย\", \"ฟื้นตัว\", \"หลัง\", \"โควิด\",\n",
    "        \"การท่องเที่ยว\", \"และ\", \"การส่งออก\", \"เป็น\", \"ปัจจัย\",\n",
    "        \"สำคัญ\", \"ใน\", \"การ\", \"ขับเคลื่อน\", \"เศรษฐกิจ\",\n",
    "        \"รัฐบาล\", \"เร่ง\", \"ออก\", \"มาตรการ\", \"กระตุ้น\"],\n",
    "        [\"นวัตกรรม\", \"ปัญญาประดิษฐ์\", \"ใน\", \"วงการ\", \"แพทย์\",\n",
    "        \"AI\", \"ช่วย\", \"วินิจฉัย\", \"โรค\", \"ได้\",\n",
    "        \"แม่นยำ\", \"ขึ้น\", \"โรงพยาบาล\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"เริ่ม\", \"นำ\", \"มา\", \"ใช้\"],\n",
    "        [\"การเปลี่ยนแปลง\", \"สภาพ\", \"ภูมิอากาศ\", \"กระทบ\", \"ภาค\",\n",
    "        \"เกษตร\", \"เกษตรกร\", \"ไทย\", \"ปรับตัว\", \"รับมือ\",\n",
    "        \"ภัยแล้ง\", \"และ\", \"น้ำท่วม\", \"นักวิทยาศาสตร์\", \"เร่ง\",\n",
    "         \"คิดค้น\", \"พันธุ์พืช\", \"ทนทาน\"],\n",
    "        [\"พลังงาน\", \"นิวเคลียร์\", \"ทางเลือก\", \"หรือ\", \"ทางตัน\",\n",
    "        \"ประเทศไทย\", \"ยัง\", \"ลังเล\", \"ใน\", \"การพัฒนา\",\n",
    "        \"โรงไฟฟ้า\", \"นิวเคลียร์\", \"ขณะที่\", \"หลาย\", \"ประเทศ\",\n",
    "        \"เดินหน้า\", \"เต็มที\"],\n",
    "        [\"การพัฒนา\", \"เมือง\", \"อัจฉริยะ\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"กรุงเทพฯ\", \"และ\", \"เมือง\", \"ใหม่\", \"เร่ง\",\n",
    "        \"ปรับตัว\", \"สู่\", \"Smart City\", \"ใช้\", \"เทคโนโลยี\",\n",
    "        \"IoT\", \"เพื่อ\", \"ยกระดับ\", \"คุณภาพ\", \"ชีวิต\"],\n",
    "        [\"วิกฤต\", \"ขยะ\", \"พลาสติก\", \"ใน\", \"ทะเลไทย\",\n",
    "        \"นักวิทยาศาสตร์\", \"เตือน\", \"ผลกระทบ\", \"ต่อ\", \"ระบบนิเวศ\",\n",
    "        \"รัฐบาล\", \"ออก\", \"มาตรการ\", \"ลด\", \"การใช้\",\n",
    "        \"พลาสติก\"],\n",
    "        [\"5G\", \"เปลี่ยน\", \"โฉม\", \"อุตสาหกรรม\", \"ไทย\",\n",
    "        \"ผู้ประกอบการ\", \"เร่ง\", \"ปรับตัว\", \"รับ\", \"เทคโนโลยี\",\n",
    "        \"ใหม่\",  \"คาด\", \"ช่วย\", \"เพิ่ม\", \"ประสิทธิภาพ\",\n",
    "        \"การผลิต\"],\n",
    "        [\"การท่องเที่ยว\", \"เชิงนิเวศ\", \"บูม\", \"ใน\", \"ไทย\",\n",
    "        \"นักท่องเที่ยว\", \"ต่างชาติ\", \"สนใจ\", \"ธรรมชาติ\", \"และ\",\n",
    "        \"วัฒนธรรม\", \"ท่องถิ่น\", \"ช่วย\", \"กระจาย\", \"รายได้\",\n",
    "        \"สู่\", \"ชุมชน\"],\n",
    "        [\"พลังงาน\", \"สะอาด\", \"กับ\", \"การพัฒนา\", \"ที่\",\n",
    "        \"ยั่งยืน\", \"ประเทศไทย\", \"ตั้งเป้า\", \"เพิ่ม\", \"สัดส่วน\",\n",
    "        \"พลังงาน\", \"หมุนเวียน\", \"นักลงทุน\", \"สนใจ\",\"ลงทุน\",\n",
    "        \"ใน\", \"โครงการ\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\",\n",
    "        \"ลม\"]\n",
    "    ]\n",
    "    target_words = [\"พลังงาน\", \"นวัตกรรม\", \"เศรษฐกิจ\", \"ประเทศไทย\", \"เทคโนโลยี\"]\n",
    "    calculator = TFIDFCalculator(documents)\n",
    "    results = calculator.calculate_tfidf(target_words)\n",
    "\n",
    "    for term, scores in results.items():\n",
    "        print(f\"\\nคำว่า '{term}':\")\n",
    "        for doc_idx, score in enumerate(scores, 1):\n",
    "            if score > 0:\n",
    "                print(f\"D{doc_idx}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([\n",
    "  2, 1, 2, 3, 2, 9\n",
    "  ])\n",
    "\n",
    "B = np.array([\n",
    "  3, 4, 2, 4, 5, 5\n",
    "  ])\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(f\"Cosine Similarity: {cosine:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity and distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rows = [\"น้ำ\", \"ข้าว\", \"ผลไม้\", \"จาน\", \"แก้ว\", \"เนื้อ\", \"ปลา\", \"ผัก\"]\n",
    "columns = [\"กิน\", \"ดื่ม\", \"ซื้อ\", \"ล้าง\", \"เก็บ\", \"ปรุง\", \"หั่น\", \"แช่\", \"ขวด\",\n",
    "           \"ถาด\", \"ชาม\", \"ถ้วย\", \"ชิ้น\", \"ผล\", \"ใน\", \"บน\", \"กับ\", \"และ\"]\n",
    "\n",
    "data = [[25, 95, 42, 38, 12, 0, 0, 85, 90, 0, 0, 65, 0, 0, 75, 0, 85, 45],\n",
    "        [82, 0, 35, 0, 45, 58, 0, 0, 0, 85, 90, 75, 0, 0, 65, 0, 78, 55],\n",
    "        [68, 52, 73, 45, 38, 0, 75, 65, 0, 25, 0, 0, 85, 95, 45, 0, 65, 85],\n",
    "        [0, 0, 28, 92, 85, 0, 0, 0, 0, 0, 72, 0, 0, 0, 0, 95, 45, 55],\n",
    "        [0, 88, 32, 75, 62, 0, 0, 0, 87, 0, 0, 0, 0, 0, 0, 85, 35, 45],\n",
    "        [81, 0, 60, 68, 56, 72, 85, 55, 0, 75, 65, 0, 95, 0, 45, 75, 85, 65],\n",
    "        [85, 0, 65, 72, 48, 78, 82, 62, 0, 78, 68, 0, 92, 0, 52, 72, 88, 58],\n",
    "        [75, 0, 70, 85, 52, 65, 88, 45, 0, 65, 55, 0, 0, 0, 35, 65, 92, 75]]\n",
    "\n",
    "df = pd.DataFrame(data, index=rows, columns=columns)\n",
    "\n",
    "def cosine_calculation(word_pair, mode = \"similarity\"):\n",
    "    vec1 = df.loc[word_pair[0]].values\n",
    "    vec2 = df.loc[word_pair[1]].values\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    result = cosine_similarity if mode == \"similarity\" else 1 - cosine_similarity\n",
    "\n",
    "    return f\"cosine {mode} ระหว่าง '{word_pair[0]}' และ '{word_pair[1]}' = {result:.4f}\"\n",
    "\n",
    "# print(cosine_calculation((\"จาน\", \"ปลา\")))\n",
    "# print(cosine_calculation((\"เนื้อ\", \"น้ำ\")))\n",
    "# print(cosine_calculation((\"ผัก\", \"ข้าว\")))\n",
    "# print(cosine_calculation((\"ผลไม้\", \"แก้ว\"), mode = \"distance\"))\n",
    "# print(cosine_calculation((\"ข้าว\", \"แก้ว\"), mode = \"distance\"))\n",
    "# print(cosine_calculation((\"ปลา\", \"แก้ว\"), mode = \"distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การวิเคราะห์ความสัมพันธ์ระหว่างคำนามกับลักษณนาม\n",
    "\n",
    "classifiers = [\"ขวด\", \"ถาด\", \"ชาม\", \"ถ้วย\", \"ชิ้น\", \"ผล\"]\n",
    "nouns = df.index\n",
    "\n",
    "classifier_df = df[classifiers]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    relation = classifier_df[classifier].max()\n",
    "    nouns = classifier_df[classifier_df[classifier] == relation].index\n",
    "    print(f\"ลักษณนาม '{classifier}' มีความสัมพันธ์กับ \\\"{','.join(nouns)}\\\" สูงสุด - {relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เปรียบเทียบการใช้ \"ใน\" และ \"บน\"\n",
    "\n",
    "prepositions = [\"ใน\", \"บน\"]\n",
    "for prep in prepositions:\n",
    "    sorted_values = df[prep].sort_values(ascending=False)\n",
    "    print(f\"\\nการใช้ '{prep}':\")\n",
    "    for noun, value in sorted_values.items():\n",
    "        if value > 0:\n",
    "            print(f\"{noun}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การจัดกลุ่มคำนามที่มีความสัมพันธ์ใกล้เคียง\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# คำนวณ cosine similarity\n",
    "def mini_cosine(word1, word2):\n",
    "    vec1 = df.loc[word1].values\n",
    "    vec2 = df.loc[word2].values\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# หาความสัมพันธ์ระหว่างทุกคู่คำ\n",
    "word_pairs = list(combinations(df.index, 2))\n",
    "similarities = [(pair[0], pair[1], mini_cosine(pair[0], pair[1])) \n",
    "               for pair in word_pairs]\n",
    "\n",
    "# เรียงลำดับและแสดงผล\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "print(\"คู่คำที่มีความสัมพันธ์ใกล้เคียงที่สุด:\")\n",
    "for pair in similarities[:5]:\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# วิเคราะห์การใช้กริยา \"กิน ดื่ม ปรุง หั่น\"\n",
    "\n",
    "verbs = [\"กิน\", \"ดื่ม\", \"ปรุง\", \"หั่น\"]\n",
    "verb_df = df[verbs]\n",
    "distances = []\n",
    "\n",
    "for noun in df.index:\n",
    "    current_vec = verb_df.loc[noun].values\n",
    "\n",
    "    other_nouns = [n for n in df.index if n != noun]\n",
    "    other_vectors = verb_df.loc[other_nouns].values\n",
    "    mean_vec = np.mean(other_vectors, axis=0)\n",
    "    \n",
    "    distance = 1 - np.dot(current_vec, mean_vec) / (np.linalg.norm(current_vec) * np.linalg.norm(mean_vec))\n",
    "    distances.append((noun, distance))\n",
    "\n",
    "distances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nความแตกต่างของรูปแบบการใช้กริยา (เรียงจากมากไปน้อย):\")\n",
    "for noun, dist in distances:\n",
    "    print(f\"{noun}: {dist:.4f}\")\n",
    "    pattern = verb_df.loc[noun]\n",
    "    # print(f\"Pattern: กิน = {pattern['กิน']}, ดื่ม = {pattern['ดื่ม']}, ปรุง = {pattern['ปรุง']}, หั่น = {pattern['หั่น']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ประโยค \"คุณแม่_น้ำใส่แก้ว\"\n",
    "\n",
    "target_verbs = df.columns[:8]  # ['กิน', 'ดื่ม', 'ซื้อ', 'ล้าง', 'เก็บ', 'ปรุง', 'หั่น', 'แช่']\n",
    "scores = []\n",
    "\n",
    "for verb in target_verbs:\n",
    "    # คำนวณ score โดยคูณค่าความสัมพันธ์ของทั้งสองคำ\n",
    "    score = df.loc[\"น้ำ\", verb] * df.loc[\"แก้ว\", verb]\n",
    "    scores.append((verb, score))\n",
    "\n",
    "# เรียงลำดับจากมากไปน้อย\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"คะแนนความเป็นไปได้ของแต่ละกริยา:\")\n",
    "for verb, score in scores:\n",
    "    if score > 0:  # แสดงเฉพาะกริยาที่มีความเป็นไปได้ (score > 0)\n",
    "        print(f\"{verb}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ความเป็นไปได้ของลักษณนามกับ \"เนื้อ\"\n",
    "\n",
    "classifiers = [\"ชิ้น\", \"ถาด\", \"ชาม\"] # เลือกเฉพาะลักษณนามที่เกี่ยวข้อง\n",
    "\n",
    "# แสดงค่าความสัมพันธ์ก่อนคำนวณความน่าจะเป็น\n",
    "print(\"ค่าความสัมพันธ์ระหว่างคำว่า 'เนื้อ' กับลักษณนามต่างๆ:\")\n",
    "for clf in classifiers:\n",
    "    print(f\"{clf}: {df.loc['เนื้อ', clf]}\")\n",
    "\n",
    "# คำนวณผลรวมของค่าความสัมพันธ์ทั้งหมด\n",
    "total = sum(df.loc[\"เนื้อ\", classifiers])\n",
    "print(f\"ผลรวมค่าความสัมพันธ์: {total}\")\n",
    "\n",
    "classifier_probs = []\n",
    "for clf in classifiers:\n",
    "    # คำนวณความน่าจะเป็นโดยหารด้วยผลรวม\n",
    "    prob = df.loc[\"เนื้อ\", clf] / total\n",
    "    classifier_probs.append((clf, prob))\n",
    "\n",
    "# เรียงลำดับความน่าจะเป็นจากมากไปน้อย\n",
    "classifier_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nความน่าจะเป็นของแต่ละลักษณนาม:\")\n",
    "for clf, prob in classifier_probs:\n",
    "    print(f\"{clf}: {prob:.4f} ({df.loc['เนื้อ', clf]}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Language in Digital Media\n",
    "**LG468 Language in Digital Media**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "\n",
    "API_KEY = 'kHIllIH4ODKsOvvi7QJINN5FIzf6sFgR'\n",
    "API_FOR_THAI = \"https://api.aiforthai.in.th\"\n",
    "SSSENSE_ENDPOINT = f\"{API_FOR_THAI}/ssense\"\n",
    "TEXT_CLEANSING_ENDPOINT = f\"{API_FOR_THAI}/textcleansing\"\n",
    "\n",
    "HEADERS = {\"apikey\": API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['อ้าว อยากมีเรื่องหรอวะ ไอ้หัวเกรียน!', 'ขอโทษที่รบกวนคะ', 'รักเธอนะ คนดี', 'เหี้ย สัตว์มึงอย่าเสือกดิ', 'ไอ้คนทรยศ แกอย่าหวังว่าจะตายดี', 'ดีใจด้วยนะ คุณหญิงกฤตยา', 'เธอเป็นความภูมิใจที่สุดของฉัน', 'ทปอ. ว่าแต่เขาอิเหนาเป็นเอง ขี้คุยชมัด', 'ติดมหิดล รอบพอร์ตแล้ว เย้ๆๆ', 'มือถือค้างอ่ะ เซ็งจัง', 'เอาหมูคุโรบุตะมาทำสเต๊กนี่ กินแล้วฟินขั้นเทพพพพ', 'เกรซไม่เคยเปิดใจให้เรา ไม่รู้เค้ามีอคติไรนักหนา', 'สอบเกือบผ่าน ตกไปแค่ 0.5 คะแนนเอง ถถถถ', 'ทักคับ 😃', 'ยินดีด้วยแนนได้ผู้แทนเลขละ มาสอนข้าน้อยด้วยด่วนๆ5555', 'น่าเสียดายอ่ะน้องแกพลาดประธานตึกไปแค่ 16 แต้ม 😭', 'น่ารักมากมายด์ ตายอย่างสงบศพสีชมพู❤️', 'หลายครั้ง ความรักก็ทำให้เราตาบอดนะ เอิร์น', 'เหม็น หยุดใช้น้ำหอมยี่ห้อนี้ได้มะ เรารู้สึกเหม็นฉุนๆ มากกว่าหอมอ่า._.', 'พระคุณที่สามงดงามแจ่มใส', 'ของเก่าจนเกือบจะใช้ไม่ได้ เหมือนอายุเจ้าของที่ใกล้ลงโลง', 'สบายดีจ้า~', '5555555555 ขำจังเลย ตลกดี ชอบ', 'เราไม่ชอบเฌอปราง แบบเกลียดขี้หน้านาง', 'แล้วจะให้ฉันทำยังไง แกทำตัวแย่ซะขนาดนี้ ไอ้ลูกทรพี', 'หิวข้าว รู้งี้กินข้าวเที่ยงดีกว่า เรื่องจริงนาจา', 'สวัสดีครับ หมาคุณน่ารักจังเลย มันชื่ออะไรหรอคับ', 'สุดที่รักเธอโทรมาหา ช่วยรับสายหน่อยสิจ้ะ', 'อย่าให้เขาต้องรอนานอย่างนี้เลย ผิดไปแล้ว 🙏', 'กระแสประสาทจึงส่งไปสมองน้อยลง ทำให้สงบไม่วิตกกังวล', 'สาบานเหอะว่าที่แกพูดนะ แกไม่รู้ความหมาย กูไม่เชื่อหรอกโว้ย', 'ขอพระองค์ทรงพระเจริญยิ่งยืนนาน', 'พระองค์ทรงเปี่ยมล้นด้วยทศพิธราชธรรม', 'ไม่แนะนำซื้อมาใช้ได้วันเดียวใช้ไม่ได้แล้ว', 'แย่ๆสั่งสินค้าไปหลายวันแล้วไม่ส่งให้สักทีเหนื่อยกับการรอคอย', 'เสียความรู้สึกอย่างมาก สวยแต่ในรูปใส่ได้ครั้งเดียว ไม่ดี', 'ยอดเยี่ยมค่ะ ราคาเหมาะสม บอกต่อได้เลยค่ะ', 'สวยค่ะ เหมือนในรูปเลย ชอบค่ะ ส่งของไวมากก', 'ใช้ดีค่ะหน้านุ่มและขาวขึ้น', 'จัดส่งเร็วมาก สะดวกดี ของจริงขนาดใหญ่กว่าที่คิดไว้ แต่ใช้งานได้ดี', 'ชอบสินค้ามาก ไม่ผิดหวังที่รอ สวยยังบรรจุภัณฑ์คับ', 'สินค้าส่งรวดเร็ว พอใจในสินค้าครับ', 'สินค้าคุณภาพดี แข็งแรง เสียงดี เด็กๆ ชอบ และยังส่งเร็วด้วยค่ะ', 'ราคากับคุณภาพของสินค้ามีความเหมาะสม ใช้งานง่าย', 'ส่งของไว ห่อพัสดุดี  ของใช้งานเยี่ยม', 'สวยค่ะ การจัดส่งก็รวดเร็วดีค่ะ', 'ใช้เดือนเดียวพังแย่ๆมากๆ', 'เสียใจความรู้สึกมากกระเป๋าบางมากไม่ตรงกะแบบสินค้าที่ลงการแพ็คของส่งก็แย่เอาถุงดำที่ใส่ขยะแถมสกปรกมากทำไมไม่ตรวจสอบก่อนส่งถึงมือลูกค้าเราไปซื้อของคุณนะไม่ได้ไปขอฟรีๆปรับปรุงด่วน!!!! ', 'การส่งของแย่มาก ขนาดตะโกนว่ารอเดี๋ยว ยังโยนสินค้าข้ามรั้ว ถ้าโน้ตบุคเสียหายจะทำยังไง', 'ใช้ในรถระบบเสีย พังใช้ต่อไม่ได้', 'ผิดหวัง ลบไม่ออก', 'หมดอายุแล้วไม่ควรเอามาลงขายค่ะเสียใจผิดหวังมากๆค่ะกว่าจะหมดแพงด้วยนะ', 'ส่งช้ามาก ตอนนี้ยังไม่ได้ของ', 'ได้รับของแล้วผิดหวังมาก ไม่ดีเลยค่ะ', 'สินค้าไม่มีคุณภาพสายกางเกงไม่เท่ากัน', 'ช้าสุดๆรอจนรำคาญละ', 'คุณภาพคุ้มค่าครับ', 'ส่งเร็วดี ใช้ดีคุ้มมาก ราคาถูก', 'ได้รับสินค้าเร็วค่ะ และสินค้าก็มีคุณภาพ สวมใส่สบายตัวมากๆๆ เลยค่ะ', 'ได้รับของเร็วมาก.. แถมสินค้าได้ตามต้องการสวยมาก', 'รับประทานดีอยู่นะครับ มองเห็นความเปลี่ยนแปลงของร่างกาย ทานแล้วรู้สึกว่าดีนะ', 'ใช้ได้ดีเหมาะกับราคานี้ ก็สรุปว่าดีครับ', 'สินค้าใช้แล้วดีมากไม่ทำให้ลูกแพ้เลยแถมราคาถูกกว่าในห้างและตามร้านค้าอีก', 'ใช้งานดีมากส่งของก็เร็วของครับ', 'ส่งไวมาก สั่งตอนเย็น ตอนเช้าได้แล้ว สินค้าแพ็คมาดีมาก ใช้งานได้ปกติไม่มีปัญหาอะไร ถือว่าคุ้มค่าสุดๆครับ', 'วัสดุค่อนข้างดีนับว่าคุ้มราคาครับ', 'สินค้าไม่เหมือนในรูป ผิดหวังมาก เย็บไม่ดีเลย', 'ใช้แล้วสิวขึ้นเต็มเลยไม่รู้แพ้รึเปล่า....เซ็งเลยเหลือตั้งเยอะเสียดายมาก', 'คุณภาพแย่มากใช้งานไม่ได้แม้แต่ครั้งเดียว', 'ผิดหวังมาก ไม่แนะนำให้ใช้ค่ะ', 'สรุปแพงกว่าท้องตลาดคะแย่มากเสียความ', 'รู้สึกผิดหวังมาก ไม่ใช่แบบที่หวังไว้เลยย', 'แย่มากๆๆๆๆ ไม่ส่งของแถมมาให้ตามที่โฆษณา ', 'งานไม่ค่อยดี ไม่สมกับราคา', 'แย่มากเลย นมใกล้หมดอายุ ครั้งนี้ครั้งสุดท้ายนะคะที่จะสั่ง', 'ใช้ได้ไม่กี่เดือนก็พังล่ะยังใช้ไม่คุ้มกับราคาที่ซื้อมาเลย', 'ใส่สบายสั่งหลายรอบแล้วจร้าาา', 'ส่งของรวดเร็วดี คุณภาพใช้ได้ค่ะ', 'ใช้งานได้ดีราคาไม่แพงมาก', 'นอกจากสงสารตัวเองแล้ว ยังสงสารแม่อีกอ่ะ เราอยากให้แม่ภูมิใจในตัวเรา เราเลยเลือกสิ่งที่คิดว่ามันจะดีกับเราในระยะยาว ทำให้เราหวังมากขึ้นไปอีก แม่ก็เครียด แม่นอนไม่หลับมาตั้งแต่คืนที่เราลุกไปสมัครสอบตอนตี4แล้ว แต่แม่ก็ต้องไปทำงานทุกวัน เราอยากติดแล้วจริงๆนะ เป็นห่วงแม่ 😭', 'ท้อจนบางทีก็นั่งๆอยู่ก็คิดว่า ไม่อยากเรียนแล้วอ่ะ', 'ที่ติดนี่คือ โล่งใจมากกว่าดีใจ เพราะเหนื่อยใจมามากแล้ว', 'ทั้งๆที่ไม่ค่อยโอเคกับรอบ3 แต่ก็เสือกสมัครเพราะอยากมีที่เนียนแล้ว... เออ กูเอง😭 เพราะความอยากมีที่เรียนของกูอ่ะ กูเลยไม่อยากไปสมัครรอบ4 แต่ก็เสือกบ่นให้รอบ3 กู กูเอ๊งงงง!!!!!', 'ติดแล้วเด้ออออ ติดแล้วจ้าาา สถานีต่อไป กูต้องแก้บน!!!!', 'ติดแล้ววววววววววขอบคุณมากนะคะ ', 'ติดแล้ววววววว ดีใจที่ตัวเองยังหน้าด้านยื่นต่อจนติด', 'ติดมศว แล้วนะ ฮื่ออออ ดีใจจจจจ', 'ตอนแรกก็ไม่ค่อยเครียดหรอก ตอนนี้เริ่มเครียดแล้วจ้า😭😭', 'ตอนนี้รอประกาศผลจนฟุ่งซานล่ะ นานเกิ้นแถมไม่รู้ว่าจะทำไรด้วย แมร่งภาวนาให้แต่ถึงวันที่ 29 เร็วๆ แมร่งโครตอึดอัดเลย', 'ตอนนี้คือเครียดเรื่องสัมภาษณ์ คือถ้าผ่าน ก็ดีไป แต่ถ้าไม่ผ่านนี่ดิ จะทำไงวะ ไหนจะวางแผนเรื่องอันดับ แล้วถ้าเข้าไปสมัครละเว็บล่ม เห้ออ เครียด ทำไมเด็ก61มันเครียดขนาดนี้ ระบบเหี้ยนี่แหละทำให้กูลุกไปไหนไม่ได้ ลุ้นตลอดเวลา เหนื่อยโว้ยยย!!!!', 'ดีใจมากกกกกกก ภูมิใจในตัวเองอ่ะ ❤️ ขอบคุณที่สู้มานะ ขอบคุณทุกกำลังใจทุกความช่วยเหลือ น้ำตาไหลเลย ', 'ดีใจด้วยกับคนที่ติดแล้ว รักทุกคนมากๆ พยายามกันมามาก ผ่านอะไรกันมาเข้าใจความรู้สึกทุกคน รู้สึกถึงความเป็นเพื่อนที่ต้อสู้กันมา รอเราหน่อยนะ รอบ4มันจะเป็นของเรา ขอให้ #dek61 ติดคณะที่ชอบ มหาลัยที่หวังไว้นะ #TCASรอบ4 🙏🏻 .เราจะไม่โดนเทรอบ4', 'ดีใจจมากกก ฮรืออ ได้ใช้แท็กนี้สักที 😭😭😭', 'ดีใจกับน้องๆทุกคนที่ติดวิศวะ จุฬาฯ ด้วยค่ะ', 'ดีใจกับคนที่ติดรอบ 3/2ด้วยนะคะ ใครที่รอแอด หรือติดแต่ไม่ถูกใจ อดใจรอกันอีกนิดนะคะ', 'จากเพจอ.ขลุ่ย ดีใจที่เห็นเพลงนี้ให้กำลังใจคนได้จริงๆ สู้ๆนะน้อง มันอาจจะเหนื่อยจนท้อบ้าง แต่สักวันมันจะเป็นวันของเรา พี่เป็นกำลังใจให้ /กอดๆ', 'จริงๆนะ คือไม่มีใครเข้าใจ #dek61 เท่าพวกเรากันเองแล้วอะ อยากนัดทุกคนไปยื่นกอดคอร้องไห้55555555 แม่งหนักจริง ว่าจะไม่เครียดแต่แม่งก็อดไม่ได้', 'จนตอนนี้หนูยังพยายามเข้าเว็บอยู่ ล่มเก่งอีดอก', 'คือแบบความรู้สึกแม่งโครตแย่อ่า ต้องคอยตอบคำถามคนนั้นคนนี้โน่นนั่นนี่ โอ้ยยยยยย ละที่สำคัญละ เครียดจนประสาทจะแดกกกกไมเกรน!!! คือหมดตังเยอะมากอ่ากับคำว่าสมัครนั่นนี่ บางที่นะแม่งบอกรอบพอตๆไม่ดูพอตดูเกรด!!! เกรดน้อยก้อแพ้ไป คือแบบแย่มากอ่า ห่วยมากเลยยย บางคนก็รอจนรากจะออกก้น คือแบบคิดมากไง กูจะมีที่เรียนป่าววะ กูอยากเรียนอันนี่ชอบอันนี้แต่ไม่ได้หวะ งั้นงี้ สารพัดปัญหา จนตอนนี้กูก็ยังไม่มีที่เรียน ไม่มีโอกาสแก้ตัว พลาดคือพลาด กูยากที่จะอธิบาย #เหนื่อยสัส', 'คือเมื่อคืนฝันดีมาก ฝันว่าได้ใส่ชุดนิสิต ละมีอวดคนอื่นอ่ะ รู้เลยว่าตอนนี้ตัวเองเครียดและหวังไว้สูงมาก']\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read().splitlines()\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def cleanse_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        response = requests.post(TEXT_CLEANSING_ENDPOINT, data={'text': text}, headers=HEADERS)\n",
    "        cleaned_data.append(response.json()['cleansing_text'])\n",
    "    return cleaned_data\n",
    "\n",
    "data = load_data(r'datasets\\test.csv')\n",
    "\n",
    "cleaned_data = cleanse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    text = []\n",
    "    polarity = []\n",
    "    confidence = []\n",
    "    keywords = []\n",
    "    poswords = []\n",
    "    negwords = []\n",
    "\n",
    "    for text_data in data:\n",
    "        response = requests.post(SSSENSE_ENDPOINT, data={'text': text_data}, headers=HEADERS)\n",
    "        if response.json()['sentiment']['score'] > '50':\n",
    "            text.append(response.json()['preprocess']['input'])\n",
    "            polarity.append(response.json()['sentiment']['polarity'])\n",
    "            confidence.append(float(response.json()['sentiment']['score']))\n",
    "            keywords.extend(response.json()['preprocess']['keyword'])\n",
    "            if response.json()['preprocess']['pos']:\n",
    "                poswords.extend(response.json()['preprocess']['pos'])\n",
    "            if response.json()['preprocess']['neg']:\n",
    "                negwords.extend(response.json()['preprocess']['neg'])\n",
    "\n",
    "    return text, polarity, confidence, keywords, poswords, negwords\n",
    "\n",
    "text, polarity, confidence, keywords, poswords, negwords = analyze_sentiment(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Output\n",
    "\n",
    "def process_data(text, polarity, confidence):\n",
    "    confidence_lst = list(zip(polarity, confidence))\n",
    "    predicted_lst = list(zip(text, polarity))\n",
    "    return confidence_lst, predicted_lst\n",
    "\n",
    "confidence_lst, predicted_lst = process_data(text, polarity, confidence)\n",
    "\n",
    "print(confidence_lst)\n",
    "print(predicted_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Sentiment', y='Confidence', data=df)\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "bins = np.linspace(50, 100, 10)\n",
    "\n",
    "df['Confidence_Range'] = pd.cut(df['Confidence'], bins=bins, include_lowest=True)\n",
    "\n",
    "pivot_df = df.pivot_table(values='Confidence', index='Confidence_Range', \n",
    "                          columns='Sentiment', aggfunc='count', fill_value=0)\n",
    "\n",
    "pivot_df = pivot_df.sort_index(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence Score Ranges\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Word Clouds\n",
    "\n",
    "text_neg = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'negative')\n",
    "text_pos = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'positive')\n",
    "\n",
    "fp = 'THSarabunNew.ttf'\n",
    "reg = r\"[ก-๙a-zA-Z']+\"\n",
    "thai_stopwords = list(thai_stopwords())\n",
    "\n",
    "wordcloud_neg = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_neg)\n",
    "\n",
    "wordcloud_pos = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_pos)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axs[0].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Negative Sentiment')\n",
    "\n",
    "axs[1].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Positive Sentiment')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
