{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pythainlp\n",
    "# !pip3 install https://github.com/PyThaiNLP/thai_sentiment_analysis/archive/master.zip\n",
    "\n",
    "!pip3 install kenlm==0.2.0\n",
    "!pip3 install pypdf==3.17.1\n",
    "!pip3 install pytesseract==0.3.10\n",
    "!pip3 install PyMuPDF==1.23.6\n",
    "!pip3 install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer\n",
    "\n",
    "text = \"สมชายเห็นชอบกลบทบาทนี้\"\n",
    "\n",
    "print(\"newmm  :\", word_tokenize(text))\n",
    "print(\"longest:\", word_tokenize(text, engine=\"longest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = {\n",
    "    \"Word\": [\"แล\", \"เบิ่ง\", \"ผ่อ\"],\n",
    "    \"POS\" : [\"ก.\", \"ก.\", \"ก.\"],\n",
    "    \"Definition\": [\"ดู มอง\", \"ดู มอง เหลียวดู\", \"ดู ดูแล มอง\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_cosine_similarity(definitions):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(definitions)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def pos_similarity(pos_list):\n",
    "    n = len(pos_list)\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += 1 if pos_list[i] == pos_list[j] else 0\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "def definition_similarity(definitions):\n",
    "    similarity_matrix = calculate_cosine_similarity(definitions)\n",
    "    n = similarity_matrix.shape[0]\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += similarity_matrix[i, j]\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "pos = df[\"POS\"].values\n",
    "pos_sim = pos_similarity(pos)\n",
    "\n",
    "definitions = df[\"Definition\"]\n",
    "def_sim = definition_similarity(definitions)\n",
    "similarity = alpha * pos_sim + beta * def_sim\n",
    "\n",
    "print(f\"POS Similarity: {pos_sim:.2f}\")\n",
    "print(f\"Definition Similarity: {def_sim:.2f}\")\n",
    "print(f\"Overall Similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class TFIDFCalculator:\n",
    "    def __init__(self, documents: List[List[str]]):\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        self.term_freq = [Counter(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "    def calculate_tf(self, term: str, doc_idx: int) -> float:\n",
    "        if self.doc_lengths[doc_idx] == 0:\n",
    "            return 0\n",
    "        return self.term_freq[doc_idx][term] / self.doc_lengths[doc_idx]\n",
    "\n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        doc_with_term = sum(1 for doc in self.documents if term in doc)\n",
    "        if doc_with_term == 0:\n",
    "            return 0\n",
    "        return math.log2(self.doc_count / doc_with_term)\n",
    "\n",
    "    def calculate_tfidf(self, terms: List[str]) -> Dict[str, List[float]]:\n",
    "        results = {}\n",
    "        for term in terms:\n",
    "            idf = self.calculate_idf(term)\n",
    "            tfidf_scores = [\n",
    "                round(self.calculate_tf(term, doc_idx) * idf, 4)\n",
    "                for doc_idx in range(self.doc_count)\n",
    "            ]\n",
    "            results[term] = tfidf_scores\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    documents = [\n",
    "        [\"นวัตกรรม\", \"พลังงาน\", \"สะอาด\", \"เพื่อ\", \"โลก\",\n",
    "        \"ยั่งยืน\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\", \"ลม\",\n",
    "        \"กำลัง\", \"เป็นที่นิยม\", \"ใน\", \"ประเทศไทย\", \"นักวิทยาศาสตร์\",\n",
    "        \"คาดว่า\", \"จะ\", \"ช่วย\", \"ลด\", \"การปล่อย\",\n",
    "        \"ก๊าซ\", \"เรือนกระจก\", \"ได้\", \"อย่างมาก\"],\n",
    "        [\"เศรษฐกิจ\", \"ไทย\", \"ฟื้นตัว\", \"หลัง\", \"โควิด\",\n",
    "        \"การท่องเที่ยว\", \"และ\", \"การส่งออก\", \"เป็น\", \"ปัจจัย\",\n",
    "        \"สำคัญ\", \"ใน\", \"การ\", \"ขับเคลื่อน\", \"เศรษฐกิจ\",\n",
    "        \"รัฐบาล\", \"เร่ง\", \"ออก\", \"มาตรการ\", \"กระตุ้น\"],\n",
    "        [\"นวัตกรรม\", \"ปัญญาประดิษฐ์\", \"ใน\", \"วงการ\", \"แพทย์\",\n",
    "        \"AI\", \"ช่วย\", \"วินิจฉัย\", \"โรค\", \"ได้\",\n",
    "        \"แม่นยำ\", \"ขึ้น\", \"โรงพยาบาล\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"เริ่ม\", \"นำ\", \"มา\", \"ใช้\"],\n",
    "        [\"การเปลี่ยนแปลง\", \"สภาพ\", \"ภูมิอากาศ\", \"กระทบ\", \"ภาค\",\n",
    "        \"เกษตร\", \"เกษตรกร\", \"ไทย\", \"ปรับตัว\", \"รับมือ\",\n",
    "        \"ภัยแล้ง\", \"และ\", \"น้ำท่วม\", \"นักวิทยาศาสตร์\", \"เร่ง\",\n",
    "         \"คิดค้น\", \"พันธุ์พืช\", \"ทนทาน\"],\n",
    "        [\"พลังงาน\", \"นิวเคลียร์\", \"ทางเลือก\", \"หรือ\", \"ทางตัน\",\n",
    "        \"ประเทศไทย\", \"ยัง\", \"ลังเล\", \"ใน\", \"การพัฒนา\",\n",
    "        \"โรงไฟฟ้า\", \"นิวเคลียร์\", \"ขณะที่\", \"หลาย\", \"ประเทศ\",\n",
    "        \"เดินหน้า\", \"เต็มที\"],\n",
    "        [\"การพัฒนา\", \"เมือง\", \"อัจฉริยะ\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"กรุงเทพฯ\", \"และ\", \"เมือง\", \"ใหม่\", \"เร่ง\",\n",
    "        \"ปรับตัว\", \"สู่\", \"Smart City\", \"ใช้\", \"เทคโนโลยี\",\n",
    "        \"IoT\", \"เพื่อ\", \"ยกระดับ\", \"คุณภาพ\", \"ชีวิต\"],\n",
    "        [\"วิกฤต\", \"ขยะ\", \"พลาสติก\", \"ใน\", \"ทะเลไทย\",\n",
    "        \"นักวิทยาศาสตร์\", \"เตือน\", \"ผลกระทบ\", \"ต่อ\", \"ระบบนิเวศ\",\n",
    "        \"รัฐบาล\", \"ออก\", \"มาตรการ\", \"ลด\", \"การใช้\",\n",
    "        \"พลาสติก\"],\n",
    "        [\"5G\", \"เปลี่ยน\", \"โฉม\", \"อุตสาหกรรม\", \"ไทย\",\n",
    "        \"ผู้ประกอบการ\", \"เร่ง\", \"ปรับตัว\", \"รับ\", \"เทคโนโลยี\",\n",
    "        \"ใหม่\",  \"คาด\", \"ช่วย\", \"เพิ่ม\", \"ประสิทธิภาพ\",\n",
    "        \"การผลิต\"],\n",
    "        [\"การท่องเที่ยว\", \"เชิงนิเวศ\", \"บูม\", \"ใน\", \"ไทย\",\n",
    "        \"นักท่องเที่ยว\", \"ต่างชาติ\", \"สนใจ\", \"ธรรมชาติ\", \"และ\",\n",
    "        \"วัฒนธรรม\", \"ท่องถิ่น\", \"ช่วย\", \"กระจาย\", \"รายได้\",\n",
    "        \"สู่\", \"ชุมชน\"],\n",
    "        [\"พลังงาน\", \"สะอาด\", \"กับ\", \"การพัฒนา\", \"ที่\",\n",
    "        \"ยั่งยืน\", \"ประเทศไทย\", \"ตั้งเป้า\", \"เพิ่ม\", \"สัดส่วน\",\n",
    "        \"พลังงาน\", \"หมุนเวียน\", \"นักลงทุน\", \"สนใจ\",\"ลงทุน\",\n",
    "        \"ใน\", \"โครงการ\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\",\n",
    "        \"ลม\"]\n",
    "    ]\n",
    "    target_words = [\"พลังงาน\", \"นวัตกรรม\", \"เศรษฐกิจ\", \"ประเทศไทย\", \"เทคโนโลยี\"]\n",
    "    calculator = TFIDFCalculator(documents)\n",
    "    results = calculator.calculate_tfidf(target_words)\n",
    "\n",
    "    for term, scores in results.items():\n",
    "        print(f\"\\nคำว่า '{term}':\")\n",
    "        for doc_idx, score in enumerate(scores, 1):\n",
    "            if score > 0:\n",
    "                print(f\"D{doc_idx}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([\n",
    "  2, 1, 2, 3, 2, 9\n",
    "  ])\n",
    "\n",
    "B = np.array([\n",
    "  3, 4, 2, 4, 5, 5\n",
    "  ])\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(f\"Cosine Similarity: {cosine:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Language in Digital Media \\n\n",
    "**LG468 Language in Digital Media**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "\n",
    "API_KEY = 'kHIllIH4ODKsOvvi7QJINN5FIzf6sFgR'\n",
    "API_FOR_THAI = \"https://api.aiforthai.in.th\"\n",
    "SSSENSE_ENDPOINT = f\"{API_FOR_THAI}/ssense\"\n",
    "TEXT_CLEANSING_ENDPOINT = f\"{API_FOR_THAI}/textcleansing\"\n",
    "\n",
    "HEADERS = {\"apikey\": API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read().splitlines()\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def cleanse_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        response = requests.post(TEXT_CLEANSING_ENDPOINT, data={'text': text}, headers=HEADERS)\n",
    "        cleaned_data.append(response.json()['cleansing_text'])\n",
    "    return cleaned_data\n",
    "\n",
    "data = load_data(r'datasets\\sample50.csv')\n",
    "\n",
    "cleaned_data = cleanse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    text = []\n",
    "    polarity = []\n",
    "    confidence = []\n",
    "    keywords = []\n",
    "    poswords = []\n",
    "    negwords = []\n",
    "\n",
    "    for text_data in data:\n",
    "        response = requests.post(SSSENSE_ENDPOINT, data={'text': text_data}, headers=HEADERS)\n",
    "        if response.json()['sentiment']['score'] > '50':\n",
    "            text.append(response.json()['preprocess']['input'])\n",
    "            polarity.append(response.json()['sentiment']['polarity'])\n",
    "            confidence.append(float(response.json()['sentiment']['score']))\n",
    "            keywords.extend(response.json()['preprocess']['keyword'])\n",
    "            if response.json()['preprocess']['pos']:\n",
    "                poswords.extend(response.json()['preprocess']['pos'])\n",
    "            if response.json()['preprocess']['neg']:\n",
    "                negwords.extend(response.json()['preprocess']['neg'])\n",
    "\n",
    "    return text, polarity, confidence, keywords, poswords, negwords\n",
    "\n",
    "text, polarity, confidence, keywords, poswords, negwords = analyze_sentiment(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Output\n",
    "\n",
    "def process_data(text, polarity, confidence):\n",
    "    confidence_lst = list(zip(polarity, confidence))\n",
    "    predicted_lst = list(zip(text, polarity))\n",
    "    return confidence_lst, predicted_lst\n",
    "\n",
    "confidence_lst, predicted_lst = process_data(text, polarity, confidence)\n",
    "\n",
    "print(confidence_lst)\n",
    "print(predicted_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Sentiment', y='Confidence', data=df)\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "bins = np.linspace(50, 100, 10)\n",
    "\n",
    "df['Confidence_Range'] = pd.cut(df['Confidence'], bins=bins, include_lowest=True)\n",
    "\n",
    "pivot_df = df.pivot_table(values='Confidence', index='Confidence_Range', \n",
    "                          columns='Sentiment', aggfunc='count', fill_value=0)\n",
    "\n",
    "pivot_df = pivot_df.sort_index(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence Score Ranges\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Word Clouds\n",
    "\n",
    "text_neg = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'negative')\n",
    "text_pos = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'positive')\n",
    "\n",
    "fp = 'THSarabunNew.ttf'\n",
    "reg = r\"[ก-๙a-zA-Z']+\"\n",
    "thai_stopwords = list(thai_stopwords())\n",
    "\n",
    "wordcloud_neg = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_neg)\n",
    "\n",
    "wordcloud_pos = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_pos)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axs[0].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Negative Sentiment')\n",
    "\n",
    "axs[1].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Positive Sentiment')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
