{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pythainlp\n",
    "# !pip3 install https://github.com/PyThaiNLP/thai_sentiment_analysis/archive/master.zip\n",
    "\n",
    "!pip3 install kenlm==0.2.0\n",
    "!pip3 install pypdf==3.17.1\n",
    "!pip3 install pytesseract==0.3.10\n",
    "!pip3 install PyMuPDF==1.23.6\n",
    "!pip3 install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer\n",
    "\n",
    "text = \"สมชายเห็นชอบกลบทบาทนี้\"\n",
    "\n",
    "print(\"newmm  :\", word_tokenize(text))\n",
    "print(\"longest:\", word_tokenize(text, engine=\"longest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = {\n",
    "    \"Word\": [\"แล\", \"เบิ่ง\", \"ผ่อ\"],\n",
    "    \"POS\" : [\"ก.\", \"ก.\", \"ก.\"],\n",
    "    \"Definition\": [\"ดู มอง\", \"ดู มอง เหลียวดู\", \"ดู ดูแล มอง\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_cosine_similarity(definitions):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(definitions)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def pos_similarity(pos_list):\n",
    "    n = len(pos_list)\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += 1 if pos_list[i] == pos_list[j] else 0\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "def definition_similarity(definitions):\n",
    "    similarity_matrix = calculate_cosine_similarity(definitions)\n",
    "    n = similarity_matrix.shape[0]\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += similarity_matrix[i, j]\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "pos = df[\"POS\"].values\n",
    "pos_sim = pos_similarity(pos)\n",
    "\n",
    "definitions = df[\"Definition\"]\n",
    "def_sim = definition_similarity(definitions)\n",
    "similarity = alpha * pos_sim + beta * def_sim\n",
    "\n",
    "print(f\"POS Similarity: {pos_sim:.2f}\")\n",
    "print(f\"Definition Similarity: {def_sim:.2f}\")\n",
    "print(f\"Overall Similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class TFIDFCalculator:\n",
    "    def __init__(self, documents: List[List[str]]):\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        self.term_freq = [Counter(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "    def calculate_tf(self, term: str, doc_idx: int) -> float:\n",
    "        if self.doc_lengths[doc_idx] == 0:\n",
    "            return 0\n",
    "        return self.term_freq[doc_idx][term] / self.doc_lengths[doc_idx]\n",
    "\n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        doc_with_term = sum(1 for doc in self.documents if term in doc)\n",
    "        if doc_with_term == 0:\n",
    "            return 0\n",
    "        return math.log2(self.doc_count / doc_with_term)\n",
    "\n",
    "    def calculate_tfidf(self, terms: List[str]) -> Dict[str, List[float]]:\n",
    "        results = {}\n",
    "        for term in terms:\n",
    "            idf = self.calculate_idf(term)\n",
    "            tfidf_scores = [\n",
    "                round(self.calculate_tf(term, doc_idx) * idf, 4)\n",
    "                for doc_idx in range(self.doc_count)\n",
    "            ]\n",
    "            results[term] = tfidf_scores\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    documents = [\n",
    "        [\"นวัตกรรม\", \"พลังงาน\", \"สะอาด\", \"เพื่อ\", \"โลก\",\n",
    "        \"ยั่งยืน\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\", \"ลม\",\n",
    "        \"กำลัง\", \"เป็นที่นิยม\", \"ใน\", \"ประเทศไทย\", \"นักวิทยาศาสตร์\",\n",
    "        \"คาดว่า\", \"จะ\", \"ช่วย\", \"ลด\", \"การปล่อย\",\n",
    "        \"ก๊าซ\", \"เรือนกระจก\", \"ได้\", \"อย่างมาก\"],\n",
    "        [\"เศรษฐกิจ\", \"ไทย\", \"ฟื้นตัว\", \"หลัง\", \"โควิด\",\n",
    "        \"การท่องเที่ยว\", \"และ\", \"การส่งออก\", \"เป็น\", \"ปัจจัย\",\n",
    "        \"สำคัญ\", \"ใน\", \"การ\", \"ขับเคลื่อน\", \"เศรษฐกิจ\",\n",
    "        \"รัฐบาล\", \"เร่ง\", \"ออก\", \"มาตรการ\", \"กระตุ้น\"],\n",
    "        [\"นวัตกรรม\", \"ปัญญาประดิษฐ์\", \"ใน\", \"วงการ\", \"แพทย์\",\n",
    "        \"AI\", \"ช่วย\", \"วินิจฉัย\", \"โรค\", \"ได้\",\n",
    "        \"แม่นยำ\", \"ขึ้น\", \"โรงพยาบาล\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"เริ่ม\", \"นำ\", \"มา\", \"ใช้\"],\n",
    "        [\"การเปลี่ยนแปลง\", \"สภาพ\", \"ภูมิอากาศ\", \"กระทบ\", \"ภาค\",\n",
    "        \"เกษตร\", \"เกษตรกร\", \"ไทย\", \"ปรับตัว\", \"รับมือ\",\n",
    "        \"ภัยแล้ง\", \"และ\", \"น้ำท่วม\", \"นักวิทยาศาสตร์\", \"เร่ง\",\n",
    "         \"คิดค้น\", \"พันธุ์พืช\", \"ทนทาน\"],\n",
    "        [\"พลังงาน\", \"นิวเคลียร์\", \"ทางเลือก\", \"หรือ\", \"ทางตัน\",\n",
    "        \"ประเทศไทย\", \"ยัง\", \"ลังเล\", \"ใน\", \"การพัฒนา\",\n",
    "        \"โรงไฟฟ้า\", \"นิวเคลียร์\", \"ขณะที่\", \"หลาย\", \"ประเทศ\",\n",
    "        \"เดินหน้า\", \"เต็มที\"],\n",
    "        [\"การพัฒนา\", \"เมือง\", \"อัจฉริยะ\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"กรุงเทพฯ\", \"และ\", \"เมือง\", \"ใหม่\", \"เร่ง\",\n",
    "        \"ปรับตัว\", \"สู่\", \"Smart City\", \"ใช้\", \"เทคโนโลยี\",\n",
    "        \"IoT\", \"เพื่อ\", \"ยกระดับ\", \"คุณภาพ\", \"ชีวิต\"],\n",
    "        [\"วิกฤต\", \"ขยะ\", \"พลาสติก\", \"ใน\", \"ทะเลไทย\",\n",
    "        \"นักวิทยาศาสตร์\", \"เตือน\", \"ผลกระทบ\", \"ต่อ\", \"ระบบนิเวศ\",\n",
    "        \"รัฐบาล\", \"ออก\", \"มาตรการ\", \"ลด\", \"การใช้\",\n",
    "        \"พลาสติก\"],\n",
    "        [\"5G\", \"เปลี่ยน\", \"โฉม\", \"อุตสาหกรรม\", \"ไทย\",\n",
    "        \"ผู้ประกอบการ\", \"เร่ง\", \"ปรับตัว\", \"รับ\", \"เทคโนโลยี\",\n",
    "        \"ใหม่\",  \"คาด\", \"ช่วย\", \"เพิ่ม\", \"ประสิทธิภาพ\",\n",
    "        \"การผลิต\"],\n",
    "        [\"การท่องเที่ยว\", \"เชิงนิเวศ\", \"บูม\", \"ใน\", \"ไทย\",\n",
    "        \"นักท่องเที่ยว\", \"ต่างชาติ\", \"สนใจ\", \"ธรรมชาติ\", \"และ\",\n",
    "        \"วัฒนธรรม\", \"ท่องถิ่น\", \"ช่วย\", \"กระจาย\", \"รายได้\",\n",
    "        \"สู่\", \"ชุมชน\"],\n",
    "        [\"พลังงาน\", \"สะอาด\", \"กับ\", \"การพัฒนา\", \"ที่\",\n",
    "        \"ยั่งยืน\", \"ประเทศไทย\", \"ตั้งเป้า\", \"เพิ่ม\", \"สัดส่วน\",\n",
    "        \"พลังงาน\", \"หมุนเวียน\", \"นักลงทุน\", \"สนใจ\",\"ลงทุน\",\n",
    "        \"ใน\", \"โครงการ\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\",\n",
    "        \"ลม\"]\n",
    "    ]\n",
    "    target_words = [\"พลังงาน\", \"นวัตกรรม\", \"เศรษฐกิจ\", \"ประเทศไทย\", \"เทคโนโลยี\"]\n",
    "    calculator = TFIDFCalculator(documents)\n",
    "    results = calculator.calculate_tfidf(target_words)\n",
    "\n",
    "    for term, scores in results.items():\n",
    "        print(f\"\\nคำว่า '{term}':\")\n",
    "        for doc_idx, score in enumerate(scores, 1):\n",
    "            if score > 0:\n",
    "                print(f\"D{doc_idx}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([\n",
    "  2, 1, 2, 3, 2, 9\n",
    "  ])\n",
    "\n",
    "B = np.array([\n",
    "  3, 4, 2, 4, 5, 5\n",
    "  ])\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(f\"Cosine Similarity: {cosine:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from pythainlp import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(r\"datasets\\general_amy.csv\", sep='\\t', names=['text', 'sentiment'])\n",
    "\n",
    "thai_stopwords = list(thai_stopwords())\n",
    "\n",
    "def text_process(text):\n",
    "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', \"ๆ\", \"ฯ\"))\n",
    "    final = word_tokenize(final)\n",
    "    final = \" \".join(word for word in final)\n",
    "    final = \" \".join(word for word in final.split() \n",
    "                     if word.lower not in thai_stopwords)\n",
    "    return final\n",
    "\n",
    "df['text_tokens'] = df['text'].apply(text_process)\n",
    "\n",
    "df.head()\n",
    "# thai_stopwords\n",
    "\n",
    "df_pos = df[df['sentiment'] == 'pos']\n",
    "pos_word_all = \" \".join(text for text in df_pos['text_tokens'])\n",
    "reg = r\"[ก-๙a-zA-Z']+\"\n",
    "fp = 'THSarabunNew.ttf'\n",
    "wordcloud = WordCloud(stopwords=thai_stopwords, background_color = 'white', max_words=2000, height = 2000, width=4000, font_path=fp, regexp=reg).generate(pos_word_all)\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythaisa import *\n",
    "\n",
    "Model = model(name = \"test\", train_dataset = datatrain)\n",
    "Model.train()\n",
    "\n",
    "print(Model.predict(\"\"\"\n",
    "                ก่อนเสือคนมองว่าเกมนี้ อาร์เซนอลมีโอกาสสูงที่จะล้างตาเสือใต้ได้สำเร็จ เนื่องจากฟอร์มการเล่นที่ดูดีกว่าเพราะปืนใหญ่สามารถยกระดับตัวเองขึ้นมาจนเป็นทีมลุ้นแชมป์แถมปีนี้\n",
    "                \"\"\"))\n",
    "\n",
    "print(f\"items: {len(datatrain)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Language in Digital Media\n",
    "- LG468 Language in Digital Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "apikey = 'kHIllIH4ODKsOvvi7QJINN5FIzf6sFgR'\n",
    "apiforthai = \"https://api.aiforthai.in.th\"\n",
    "\n",
    "ssense = f\"{apiforthai}/ssense\"\n",
    "text_cleansing = f\"{apiforthai}/textcleansing\"\n",
    "\n",
    "with open(r'datasets\\sample1.csv', 'r', encoding='utf-8') as df:\n",
    "    lst = df.read().splitlines()\n",
    "\n",
    "headers = {\"apikey\": apikey}\n",
    "\n",
    "cleaned_lst = []\n",
    "\n",
    "def data_cleansing():\n",
    "    data = { 'text': '' }\n",
    "    i = 0\n",
    "    for i in range(len(lst)):\n",
    "        data['text'] = lst[i]\n",
    "        i += 1\n",
    "        response = requests.post(text_cleansing, data=data, headers=headers)\n",
    "        cleaned_lst.append(response.json()['cleansing_text'])\n",
    "data_cleansing()\n",
    "\n",
    "value = []\n",
    "polarity = []\n",
    "keywords = []\n",
    "poswords = []\n",
    "negwords = []\n",
    "\n",
    "def sentiment_analysis():\n",
    "    data = { 'text': '' }\n",
    "    i = 0\n",
    "    for i in range(len(cleaned_lst)):\n",
    "        data['text'] = cleaned_lst[i]\n",
    "        i += 1\n",
    "        response = requests.post(ssense, data=data, headers=headers)\n",
    "        if response.json()['sentiment']['score'] > '50':\n",
    "            polarity.append(response.json()['sentiment']['polarity'])\n",
    "            value.append(float(response.json()['sentiment']['score']))\n",
    "            for i in response.json()['preprocess']['keyword']:\n",
    "                keywords.append(i)\n",
    "            if response.json()['preprocess']['pos']:\n",
    "                for i in response.json()['preprocess']['pos']:\n",
    "                    poswords.append(i)\n",
    "            if response.json()['preprocess']['neg']:\n",
    "                for i in response.json()['preprocess']['neg']:\n",
    "                    negwords.append(i)\n",
    "sentiment_analysis()\n",
    "\n",
    "sentiment_lst = list(zip(polarity, value))\n",
    "\n",
    "print(sentiment_lst)\n",
    "print(keywords)\n",
    "print(poswords)\n",
    "print(negwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(sentiment_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.violinplot(x='Sentiment', y='Confidence', data=df)\n",
    "plt.title(\"Violin Plot of Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence (%)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Sentiment', y='Confidence', data=df, palette=\"coolwarm\")\n",
    "plt.title(\"Confidence Score Distribution by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
