{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pythainlp\n",
    "# !pip3 install https://github.com/PyThaiNLP/thai_sentiment_analysis/archive/master.zip\n",
    "# !pip3 install kenlm==0.2.0\n",
    "# !pip3 install pypdf==3.17.1\n",
    "# !pip3 install pytesseract==0.3.10\n",
    "# !pip3 install PyMuPDF==1.23.6\n",
    "# !pip3 install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize, Tokenizer\n",
    "\n",
    "text = \"สมชายเห็นชอบกลบทบาทนี้\"\n",
    "\n",
    "print(\"newmm  :\", word_tokenize(text))\n",
    "print(\"longest:\", word_tokenize(text, engine=\"longest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = {\n",
    "    \"Word\": [\"แล\", \"เบิ่ง\", \"ผ่อ\"],\n",
    "    \"POS\" : [\"ก.\", \"ก.\", \"ก.\"],\n",
    "    \"Definition\": [\"ดู มอง\", \"ดู มอง เหลียวดู\", \"ดู ดูแล มอง\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_cosine_similarity(definitions):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(definitions)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def pos_similarity(pos_list):\n",
    "    n = len(pos_list)\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += 1 if pos_list[i] == pos_list[j] else 0\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "def definition_similarity(definitions):\n",
    "    similarity_matrix = calculate_cosine_similarity(definitions)\n",
    "    n = similarity_matrix.shape[0]\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_sim += similarity_matrix[i, j]\n",
    "            count += 1\n",
    "    return total_sim / count if count > 0 else 0\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "pos = df[\"POS\"].values\n",
    "pos_sim = pos_similarity(pos)\n",
    "\n",
    "definitions = df[\"Definition\"]\n",
    "def_sim = definition_similarity(definitions)\n",
    "similarity = alpha * pos_sim + beta * def_sim\n",
    "\n",
    "print(f\"POS Similarity: {pos_sim:.2f}\")\n",
    "print(f\"Definition Similarity: {def_sim:.2f}\")\n",
    "print(f\"Overall Similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class TFIDFCalculator:\n",
    "    def __init__(self, documents: List[List[str]]):\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        self.term_freq = [Counter(doc) for doc in documents]\n",
    "        self.doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "    def calculate_tf(self, term: str, doc_idx: int) -> float:\n",
    "        if self.doc_lengths[doc_idx] == 0:\n",
    "            return 0\n",
    "        return self.term_freq[doc_idx][term] / self.doc_lengths[doc_idx]\n",
    "\n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        doc_with_term = sum(1 for doc in self.documents if term in doc)\n",
    "        if doc_with_term == 0:\n",
    "            return 0\n",
    "        return math.log2(self.doc_count / doc_with_term)\n",
    "\n",
    "    def calculate_tfidf(self, terms: List[str]) -> Dict[str, List[float]]:\n",
    "        results = {}\n",
    "        for term in terms:\n",
    "            idf = self.calculate_idf(term)\n",
    "            tfidf_scores = [\n",
    "                round(self.calculate_tf(term, doc_idx) * idf, 4)\n",
    "                for doc_idx in range(self.doc_count)\n",
    "            ]\n",
    "            results[term] = tfidf_scores\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    documents = [\n",
    "        [\"นวัตกรรม\", \"พลังงาน\", \"สะอาด\", \"เพื่อ\", \"โลก\",\n",
    "        \"ยั่งยืน\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\", \"ลม\",\n",
    "        \"กำลัง\", \"เป็นที่นิยม\", \"ใน\", \"ประเทศไทย\", \"นักวิทยาศาสตร์\",\n",
    "        \"คาดว่า\", \"จะ\", \"ช่วย\", \"ลด\", \"การปล่อย\",\n",
    "        \"ก๊าซ\", \"เรือนกระจก\", \"ได้\", \"อย่างมาก\"],\n",
    "        [\"เศรษฐกิจ\", \"ไทย\", \"ฟื้นตัว\", \"หลัง\", \"โควิด\",\n",
    "        \"การท่องเที่ยว\", \"และ\", \"การส่งออก\", \"เป็น\", \"ปัจจัย\",\n",
    "        \"สำคัญ\", \"ใน\", \"การ\", \"ขับเคลื่อน\", \"เศรษฐกิจ\",\n",
    "        \"รัฐบาล\", \"เร่ง\", \"ออก\", \"มาตรการ\", \"กระตุ้น\"],\n",
    "        [\"นวัตกรรม\", \"ปัญญาประดิษฐ์\", \"ใน\", \"วงการ\", \"แพทย์\",\n",
    "        \"AI\", \"ช่วย\", \"วินิจฉัย\", \"โรค\", \"ได้\",\n",
    "        \"แม่นยำ\", \"ขึ้น\", \"โรงพยาบาล\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"เริ่ม\", \"นำ\", \"มา\", \"ใช้\"],\n",
    "        [\"การเปลี่ยนแปลง\", \"สภาพ\", \"ภูมิอากาศ\", \"กระทบ\", \"ภาค\",\n",
    "        \"เกษตร\", \"เกษตรกร\", \"ไทย\", \"ปรับตัว\", \"รับมือ\",\n",
    "        \"ภัยแล้ง\", \"และ\", \"น้ำท่วม\", \"นักวิทยาศาสตร์\", \"เร่ง\",\n",
    "         \"คิดค้น\", \"พันธุ์พืช\", \"ทนทาน\"],\n",
    "        [\"พลังงาน\", \"นิวเคลียร์\", \"ทางเลือก\", \"หรือ\", \"ทางตัน\",\n",
    "        \"ประเทศไทย\", \"ยัง\", \"ลังเล\", \"ใน\", \"การพัฒนา\",\n",
    "        \"โรงไฟฟ้า\", \"นิวเคลียร์\", \"ขณะที่\", \"หลาย\", \"ประเทศ\",\n",
    "        \"เดินหน้า\", \"เต็มที\"],\n",
    "        [\"การพัฒนา\", \"เมือง\", \"อัจฉริยะ\", \"ใน\", \"ประเทศไทย\",\n",
    "        \"กรุงเทพฯ\", \"และ\", \"เมือง\", \"ใหม่\", \"เร่ง\",\n",
    "        \"ปรับตัว\", \"สู่\", \"Smart City\", \"ใช้\", \"เทคโนโลยี\",\n",
    "        \"IoT\", \"เพื่อ\", \"ยกระดับ\", \"คุณภาพ\", \"ชีวิต\"],\n",
    "        [\"วิกฤต\", \"ขยะ\", \"พลาสติก\", \"ใน\", \"ทะเลไทย\",\n",
    "        \"นักวิทยาศาสตร์\", \"เตือน\", \"ผลกระทบ\", \"ต่อ\", \"ระบบนิเวศ\",\n",
    "        \"รัฐบาล\", \"ออก\", \"มาตรการ\", \"ลด\", \"การใช้\",\n",
    "        \"พลาสติก\"],\n",
    "        [\"5G\", \"เปลี่ยน\", \"โฉม\", \"อุตสาหกรรม\", \"ไทย\",\n",
    "        \"ผู้ประกอบการ\", \"เร่ง\", \"ปรับตัว\", \"รับ\", \"เทคโนโลยี\",\n",
    "        \"ใหม่\",  \"คาด\", \"ช่วย\", \"เพิ่ม\", \"ประสิทธิภาพ\",\n",
    "        \"การผลิต\"],\n",
    "        [\"การท่องเที่ยว\", \"เชิงนิเวศ\", \"บูม\", \"ใน\", \"ไทย\",\n",
    "        \"นักท่องเที่ยว\", \"ต่างชาติ\", \"สนใจ\", \"ธรรมชาติ\", \"และ\",\n",
    "        \"วัฒนธรรม\", \"ท่องถิ่น\", \"ช่วย\", \"กระจาย\", \"รายได้\",\n",
    "        \"สู่\", \"ชุมชน\"],\n",
    "        [\"พลังงาน\", \"สะอาด\", \"กับ\", \"การพัฒนา\", \"ที่\",\n",
    "        \"ยั่งยืน\", \"ประเทศไทย\", \"ตั้งเป้า\", \"เพิ่ม\", \"สัดส่วน\",\n",
    "        \"พลังงาน\", \"หมุนเวียน\", \"นักลงทุน\", \"สนใจ\",\"ลงทุน\",\n",
    "        \"ใน\", \"โครงการ\", \"พลังงาน\", \"แสงอาทิตย์\", \"และ\",\n",
    "        \"ลม\"]\n",
    "    ]\n",
    "    target_words = [\"พลังงาน\", \"นวัตกรรม\", \"เศรษฐกิจ\", \"ประเทศไทย\", \"เทคโนโลยี\"]\n",
    "    calculator = TFIDFCalculator(documents)\n",
    "    results = calculator.calculate_tfidf(target_words)\n",
    "\n",
    "    for term, scores in results.items():\n",
    "        print(f\"\\nคำว่า '{term}':\")\n",
    "        for doc_idx, score in enumerate(scores, 1):\n",
    "            if score > 0:\n",
    "                print(f\"D{doc_idx}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([\n",
    "  2, 1, 2, 3, 2, 9\n",
    "  ])\n",
    "\n",
    "B = np.array([\n",
    "  3, 4, 2, 4, 5, 5\n",
    "  ])\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(f\"Cosine Similarity: {cosine:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity and distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rows = [\"น้ำ\", \"ข้าว\", \"ผลไม้\", \"จาน\", \"แก้ว\", \"เนื้อ\", \"ปลา\", \"ผัก\"]\n",
    "columns = [\"กิน\", \"ดื่ม\", \"ซื้อ\", \"ล้าง\", \"เก็บ\", \"ปรุง\", \"หั่น\", \"แช่\", \"ขวด\",\n",
    "           \"ถาด\", \"ชาม\", \"ถ้วย\", \"ชิ้น\", \"ผล\", \"ใน\", \"บน\", \"กับ\", \"และ\"]\n",
    "\n",
    "data = [[25, 95, 42, 38, 12, 0, 0, 85, 90, 0, 0, 65, 0, 0, 75, 0, 85, 45],\n",
    "        [82, 0, 35, 0, 45, 58, 0, 0, 0, 85, 90, 75, 0, 0, 65, 0, 78, 55],\n",
    "        [68, 52, 73, 45, 38, 0, 75, 65, 0, 25, 0, 0, 85, 95, 45, 0, 65, 85],\n",
    "        [0, 0, 28, 92, 85, 0, 0, 0, 0, 0, 72, 0, 0, 0, 0, 95, 45, 55],\n",
    "        [0, 88, 32, 75, 62, 0, 0, 0, 87, 0, 0, 0, 0, 0, 0, 85, 35, 45],\n",
    "        [81, 0, 60, 68, 56, 72, 85, 55, 0, 75, 65, 0, 95, 0, 45, 75, 85, 65],\n",
    "        [85, 0, 65, 72, 48, 78, 82, 62, 0, 78, 68, 0, 92, 0, 52, 72, 88, 58],\n",
    "        [75, 0, 70, 85, 52, 65, 88, 45, 0, 65, 55, 0, 0, 0, 35, 65, 92, 75]]\n",
    "\n",
    "df = pd.DataFrame(data, index=rows, columns=columns)\n",
    "\n",
    "def cosine_calculation(word_pair, mode = \"similarity\"):\n",
    "    vec1 = df.loc[word_pair[0]].values\n",
    "    vec2 = df.loc[word_pair[1]].values\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    result = cosine_similarity if mode == \"similarity\" else 1 - cosine_similarity\n",
    "\n",
    "    return f\"cosine {mode} ระหว่าง '{word_pair[0]}' และ '{word_pair[1]}' = {result:.4f}\"\n",
    "\n",
    "# print(cosine_calculation((\"จาน\", \"ปลา\")))\n",
    "# print(cosine_calculation((\"เนื้อ\", \"น้ำ\")))\n",
    "# print(cosine_calculation((\"ผัก\", \"ข้าว\")))\n",
    "# print(cosine_calculation((\"ผลไม้\", \"แก้ว\"), mode = \"distance\"))\n",
    "# print(cosine_calculation((\"ข้าว\", \"แก้ว\"), mode = \"distance\"))\n",
    "# print(cosine_calculation((\"ปลา\", \"แก้ว\"), mode = \"distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การวิเคราะห์ความสัมพันธ์ระหว่างคำนามกับลักษณนาม\n",
    "\n",
    "classifiers = [\"ขวด\", \"ถาด\", \"ชาม\", \"ถ้วย\", \"ชิ้น\", \"ผล\"]\n",
    "nouns = df.index\n",
    "\n",
    "classifier_df = df[classifiers]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    relation = classifier_df[classifier].max()\n",
    "    nouns = classifier_df[classifier_df[classifier] == relation].index\n",
    "    print(f\"ลักษณนาม '{classifier}' มีความสัมพันธ์กับ \\\"{','.join(nouns)}\\\" สูงสุด - {relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เปรียบเทียบการใช้ \"ใน\" และ \"บน\"\n",
    "\n",
    "prepositions = [\"ใน\", \"บน\"]\n",
    "for prep in prepositions:\n",
    "    sorted_values = df[prep].sort_values(ascending=False)\n",
    "    print(f\"\\nการใช้ '{prep}':\")\n",
    "    for noun, value in sorted_values.items():\n",
    "        if value > 0:\n",
    "            print(f\"{noun}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# การจัดกลุ่มคำนามที่มีความสัมพันธ์ใกล้เคียง\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# คำนวณ cosine similarity\n",
    "def mini_cosine(word1, word2):\n",
    "    vec1 = df.loc[word1].values\n",
    "    vec2 = df.loc[word2].values\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# หาความสัมพันธ์ระหว่างทุกคู่คำ\n",
    "word_pairs = list(combinations(df.index, 2))\n",
    "similarities = [(pair[0], pair[1], mini_cosine(pair[0], pair[1])) \n",
    "               for pair in word_pairs]\n",
    "\n",
    "# เรียงลำดับและแสดงผล\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "print(\"คู่คำที่มีความสัมพันธ์ใกล้เคียงที่สุด:\")\n",
    "for pair in similarities[:5]:\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# วิเคราะห์การใช้กริยา \"กิน ดื่ม ปรุง หั่น\"\n",
    "\n",
    "verbs = [\"กิน\", \"ดื่ม\", \"ปรุง\", \"หั่น\"]\n",
    "verb_df = df[verbs]\n",
    "distances = []\n",
    "\n",
    "for noun in df.index:\n",
    "    current_vec = verb_df.loc[noun].values\n",
    "\n",
    "    other_nouns = [n for n in df.index if n != noun]\n",
    "    other_vectors = verb_df.loc[other_nouns].values\n",
    "    mean_vec = np.mean(other_vectors, axis=0)\n",
    "    \n",
    "    distance = 1 - np.dot(current_vec, mean_vec) / (np.linalg.norm(current_vec) * np.linalg.norm(mean_vec))\n",
    "    distances.append((noun, distance))\n",
    "\n",
    "distances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nความแตกต่างของรูปแบบการใช้กริยา (เรียงจากมากไปน้อย):\")\n",
    "for noun, dist in distances:\n",
    "    print(f\"{noun}: {dist:.4f}\")\n",
    "    pattern = verb_df.loc[noun]\n",
    "    # print(f\"Pattern: กิน = {pattern['กิน']}, ดื่ม = {pattern['ดื่ม']}, ปรุง = {pattern['ปรุง']}, หั่น = {pattern['หั่น']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ประโยค \"คุณแม่_น้ำใส่แก้ว\"\n",
    "\n",
    "target_verbs = df.columns[:8]  # ['กิน', 'ดื่ม', 'ซื้อ', 'ล้าง', 'เก็บ', 'ปรุง', 'หั่น', 'แช่']\n",
    "scores = []\n",
    "\n",
    "for verb in target_verbs:\n",
    "    # คำนวณ score โดยคูณค่าความสัมพันธ์ของทั้งสองคำ\n",
    "    score = df.loc[\"น้ำ\", verb] * df.loc[\"แก้ว\", verb]\n",
    "    scores.append((verb, score))\n",
    "\n",
    "# เรียงลำดับจากมากไปน้อย\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"คะแนนความเป็นไปได้ของแต่ละกริยา:\")\n",
    "for verb, score in scores:\n",
    "    if score > 0:  # แสดงเฉพาะกริยาที่มีความเป็นไปได้ (score > 0)\n",
    "        print(f\"{verb}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ความเป็นไปได้ของลักษณนามกับ \"เนื้อ\"\n",
    "\n",
    "classifiers = [\"ชิ้น\", \"ถาด\", \"ชาม\"] # เลือกเฉพาะลักษณนามที่เกี่ยวข้อง\n",
    "\n",
    "# แสดงค่าความสัมพันธ์ก่อนคำนวณความน่าจะเป็น\n",
    "print(\"ค่าความสัมพันธ์ระหว่างคำว่า 'เนื้อ' กับลักษณนามต่างๆ:\")\n",
    "for clf in classifiers:\n",
    "    print(f\"{clf}: {df.loc['เนื้อ', clf]}\")\n",
    "\n",
    "# คำนวณผลรวมของค่าความสัมพันธ์ทั้งหมด\n",
    "total = sum(df.loc[\"เนื้อ\", classifiers])\n",
    "print(f\"ผลรวมค่าความสัมพันธ์: {total}\")\n",
    "\n",
    "classifier_probs = []\n",
    "for clf in classifiers:\n",
    "    # คำนวณความน่าจะเป็นโดยหารด้วยผลรวม\n",
    "    prob = df.loc[\"เนื้อ\", clf] / total\n",
    "    classifier_probs.append((clf, prob))\n",
    "\n",
    "# เรียงลำดับความน่าจะเป็นจากมากไปน้อย\n",
    "classifier_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nความน่าจะเป็นของแต่ละลักษณนาม:\")\n",
    "for clf, prob in classifier_probs:\n",
    "    print(f\"{clf}: {prob:.4f} ({df.loc['เนื้อ', clf]}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -q sacrebleu==1.2.10 torch pythainlp==2.1.4 mosestokenizer sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#SOURCE\n",
    "word2vec = {}\n",
    "word2vec[\"I\"]         = torch.tensor([4, 2, 1])\n",
    "word2vec[\"love\"]      = torch.tensor([1, 1, 0])\n",
    "word2vec[\"TU\"]        = torch.tensor([1, 2, -1])\n",
    "\n",
    "# Target\n",
    "word2vec[\"ฉัน\"]        = torch.tensor([4, 10, -4])  # Target 0\n",
    "word2vec[\"ธรรมศาสตร์\"] = torch.tensor([1, 1, 9])    # Target 2\n",
    "word2vec[\"รัก\"]        = torch.tensor([6, 1, 9])    # Target 1\n",
    "\n",
    "#Target Dictionary\n",
    "target = { 0 : \"ฉัน\" , 1 : \"รัก\" , 2 : \"ธรรมศาสตร์\"}\n",
    "\n",
    "W = torch.tensor([[2, 0, 1],\n",
    "                  [0, 2, 1],\n",
    "                  [0, 1, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER tensor([6, 5, 0])\n",
      "OUTPUT tensor([12, 10, 11])\n",
      "0\n",
      "ENCODER tensor([10, 15, -4])\n",
      "OUTPUT tensor([20, 26,  9])\n",
      "1\n",
      "ENCODER tensor([16, 16,  5])\n",
      "OUTPUT tensor([32, 37, 52])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "\n",
    "encoder = word2vec[\"I\"] + word2vec[\"love\"] + word2vec[\"TU\"]\n",
    "print(\"ENCODER\",encoder)\n",
    "z = torch.matmul(encoder,W)\n",
    "print(\"OUTPUT\", z)\n",
    "print(torch.argmax(z).item())\n",
    "\n",
    "encoder = encoder + word2vec[target[torch.argmax(z).item()]]\n",
    "print(\"ENCODER\",encoder)\n",
    "z = torch.matmul(encoder,W)\n",
    "print(\"OUTPUT\",z)\n",
    "print(torch.argmax(z).item())\n",
    "\n",
    "encoder = encoder + word2vec[target[torch.argmax(z).item()]]\n",
    "print(\"ENCODER\",encoder)\n",
    "z = torch.matmul(encoder,W)\n",
    "print(\"OUTPUT\",z)\n",
    "print(torch.argmax(z).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER tensor([6, 5, 0])\n",
      "OUTPUT tensor([12, 10, 11])\n",
      "0\n",
      "ENCODER tensor([10, 15, -4])\n",
      "OUTPUT tensor([20, 26,  9])\n",
      "1\n",
      "ENCODER tensor([16, 16,  5])\n",
      "OUTPUT tensor([32, 37, 52])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# refactored\n",
    "\n",
    "word2vec = {\n",
    "    \"I\": torch.tensor([4, 2, 1]),\n",
    "    \"love\": torch.tensor([1, 1, 0]),\n",
    "    \"TU\": torch.tensor([1, 2, -1]),\n",
    "    \n",
    "    \"ฉัน\": torch.tensor([4, 10, -4]),      # Target 0\n",
    "    \"รัก\": torch.tensor([6, 1, 9]),        # Target 1\n",
    "    \"ธรรมศาสตร์\": torch.tensor([1, 1, 9])  # Target 2\n",
    "}\n",
    "\n",
    "target = {0: \"ฉัน\", 1: \"รัก\", 2: \"ธรรมศาสตร์\"}\n",
    "\n",
    "W = torch.tensor([\n",
    "    [2, 0, 1],\n",
    "    [0, 2, 1],\n",
    "    [0, 1, 4]\n",
    "])\n",
    "\n",
    "def encode_step(encoder, W):\n",
    "    z = torch.matmul(encoder, W)\n",
    "    pred_idx = torch.argmax(z).item()\n",
    "    print(\"OUTPUT\", z)\n",
    "    print(pred_idx)\n",
    "    return pred_idx\n",
    "\n",
    "encoder = word2vec[\"I\"] + word2vec[\"love\"] + word2vec[\"TU\"]\n",
    "print(\"ENCODER\", encoder)\n",
    "i = W.size(dim=1)\n",
    "\n",
    "for _ in range(i):\n",
    "    pred_idx = encode_step(encoder, W)\n",
    "    encoder = encoder + word2vec[target[pred_idx]]\n",
    "    if _ < i-1: print(\"ENCODER\", encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love TU\n",
      "Output: ฉัน รัก ธรรมศาสตร์\n"
     ]
    }
   ],
   "source": [
    "# Preset\n",
    "\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "\n",
    "class WordVectorEncoder:\n",
    "    def __init__(self, source_vectors: Dict[str, torch.Tensor], \n",
    "                 target_vectors: Dict[str, torch.Tensor], \n",
    "                 transform_matrix: torch.Tensor):\n",
    "\n",
    "        self.source_vectors = source_vectors\n",
    "        self.target_vectors = target_vectors\n",
    "        self.W = transform_matrix\n",
    "        \n",
    "        self.target_index = {i: word for i, word in enumerate(target_vectors.keys())}\n",
    "        \n",
    "        vector_size = next(iter(source_vectors.values())).size(0)\n",
    "        assert all(v.size(0) == vector_size for v in source_vectors.values()), \"All source vectors must have same size\"\n",
    "        assert all(v.size(0) == vector_size for v in target_vectors.values()), \"All target vectors must have same size\"\n",
    "        assert transform_matrix.size(0) == vector_size, \"Transform matrix input dimension must match vector size\"\n",
    "    \n",
    "    def encode_sequence(self, words: List[str], max_steps: int = 3) -> List[str]:\n",
    "        encoder = torch.zeros_like(next(iter(self.source_vectors.values())))\n",
    "        for word in words:\n",
    "            encoder += self.source_vectors[word]\n",
    "            \n",
    "        result_sequence = []\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # Transform and get prediction\n",
    "            z = torch.matmul(encoder, self.W)\n",
    "            pred_idx = torch.argmax(z).item()\n",
    "            pred_word = self.target_index[pred_idx]\n",
    "            \n",
    "            # Add to results and update encoder\n",
    "            result_sequence.append(pred_word)\n",
    "            encoder += self.target_vectors[pred_word]\n",
    "            \n",
    "        return result_sequence\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    source_vectors = {\n",
    "        \"I\": torch.tensor([4, 2, 1]),\n",
    "        \"love\": torch.tensor([1, 1, 0]),\n",
    "        \"TU\": torch.tensor([1, 2, -1])\n",
    "    }\n",
    "    \n",
    "    # Target vectors\n",
    "    target_vectors = {\n",
    "        \"ฉัน\": torch.tensor([4, 10, -4]),\n",
    "        \"รัก\": torch.tensor([6, 1, 9]),\n",
    "        \"ธรรมศาสตร์\": torch.tensor([1, 1, 9])\n",
    "    }\n",
    "    \n",
    "    # Transform matrix\n",
    "    W = torch.tensor([\n",
    "        [2, 0, 1],\n",
    "        [0, 2, 1],\n",
    "        [0, 1, 4]\n",
    "    ])\n",
    "    \n",
    "    encoder = WordVectorEncoder(source_vectors, target_vectors, W)\n",
    "    \n",
    "    input_sequence = [\"I\", \"love\", \"TU\"]\n",
    "    output_sequence = encoder.encode_sequence(input_sequence)\n",
    "    \n",
    "    print(\"Input:\", \" \".join(input_sequence))\n",
    "    print(\"Output:\", \" \".join(output_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER tensor([10, 11])\n",
      "OUTPUT tensor([10, 12])\n",
      "1\n",
      "ENCODER tensor([16, 12])\n",
      "OUTPUT tensor([16,  8])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Assignment\n",
    "\n",
    "import torch\n",
    "\n",
    "word2vec = {\n",
    "    \"LANFOON\": torch.tensor([2, 1]),\n",
    "    \"LEEDOO\": torch.tensor([1, 4]),\n",
    "    \n",
    "    \"Dream\": torch.tensor([4, 10]),      # Target 0\n",
    "    \"Good\": torch.tensor([6, 1]),        # Target 1\n",
    "}\n",
    "\n",
    "target = {0: \"Dream\", 1: \"Good\"}\n",
    "\n",
    "W = torch.tensor([[1, -1],\n",
    "                  [0, 2]])\n",
    "\n",
    "def encode_step(encoder, W):\n",
    "    z = torch.matmul(encoder, W)\n",
    "    pred_idx = torch.argmax(z).item()\n",
    "    print(\"OUTPUT\", z)\n",
    "    print(pred_idx)\n",
    "    return pred_idx\n",
    "\n",
    "encoder = word2vec[\"Good\"] + word2vec[\"Dream\"]\n",
    "print(\"ENCODER\", encoder)\n",
    "i = W.size(dim=1)\n",
    "\n",
    "for _ in range(i):\n",
    "    pred_idx = encode_step(encoder, W)\n",
    "    encoder = encoder + word2vec[target[pred_idx]]\n",
    "    if _ < i-1: print(\"ENCODER\", encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Language in Digital Media\n",
    "**LG468 Language in Digital Media**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "\n",
    "API_KEY = 'kHIllIH4ODKsOvvi7QJINN5FIzf6sFgR'\n",
    "API_FOR_THAI = \"https://api.aiforthai.in.th\"\n",
    "SSSENSE_ENDPOINT = f\"{API_FOR_THAI}/ssense\"\n",
    "TEXT_CLEANSING_ENDPOINT = f\"{API_FOR_THAI}/textcleansing\"\n",
    "\n",
    "HEADERS = {\"apikey\": API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read().splitlines()\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def cleanse_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        response = requests.post(TEXT_CLEANSING_ENDPOINT, data={'text': text}, headers=HEADERS)\n",
    "        cleaned_data.append(response.json()['cleansing_text'])\n",
    "    return cleaned_data\n",
    "\n",
    "data = load_data(r'datasets\\test.csv')\n",
    "\n",
    "cleaned_data = cleanse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    text = []\n",
    "    polarity = []\n",
    "    confidence = []\n",
    "    keywords = []\n",
    "    poswords = []\n",
    "    negwords = []\n",
    "\n",
    "    for text_data in data:\n",
    "        response = requests.post(SSSENSE_ENDPOINT, data={'text': text_data}, headers=HEADERS)\n",
    "        if response.json()['sentiment']['score'] > '50':\n",
    "            text.append(response.json()['preprocess']['input'])\n",
    "            polarity.append(response.json()['sentiment']['polarity'])\n",
    "            confidence.append(float(response.json()['sentiment']['score']))\n",
    "            keywords.extend(response.json()['preprocess']['keyword'])\n",
    "            if response.json()['preprocess']['pos']:\n",
    "                poswords.extend(response.json()['preprocess']['pos'])\n",
    "            if response.json()['preprocess']['neg']:\n",
    "                negwords.extend(response.json()['preprocess']['neg'])\n",
    "\n",
    "    return text, polarity, confidence, keywords, poswords, negwords\n",
    "\n",
    "text, polarity, confidence, keywords, poswords, negwords = analyze_sentiment(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Output\n",
    "\n",
    "def process_data(text, polarity, confidence):\n",
    "    confidence_lst = list(zip(polarity, confidence))\n",
    "    predicted_lst = list(zip(text, polarity))\n",
    "    return confidence_lst, predicted_lst\n",
    "\n",
    "confidence_lst, predicted_lst = process_data(text, polarity, confidence)\n",
    "\n",
    "print(confidence_lst)\n",
    "print(predicted_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Sentiment', y='Confidence', data=df)\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confidence_lst, columns=['Sentiment', 'Confidence'])\n",
    "\n",
    "bins = np.linspace(50, 100, 10)\n",
    "\n",
    "df['Confidence_Range'] = pd.cut(df['Confidence'], bins=bins, include_lowest=True)\n",
    "\n",
    "pivot_df = df.pivot_table(values='Confidence', index='Confidence_Range', \n",
    "                          columns='Sentiment', aggfunc='count', fill_value=0)\n",
    "\n",
    "pivot_df = pivot_df.sort_index(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title(\"Confidence Scores by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Confidence Score Ranges\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Word Clouds\n",
    "\n",
    "text_neg = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'negative')\n",
    "text_pos = \" \".join(text for text, sentiment in predicted_lst if sentiment == 'positive')\n",
    "\n",
    "fp = 'THSarabunNew.ttf'\n",
    "reg = r\"[ก-๙a-zA-Z']+\"\n",
    "thai_stopwords = list(thai_stopwords())\n",
    "\n",
    "wordcloud_neg = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_neg)\n",
    "\n",
    "wordcloud_pos = WordCloud(stopwords=thai_stopwords, background_color='white', max_words=2000,\n",
    "                          height=2000, width=4000, font_path=fp, regexp=reg).generate(text_pos)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axs[0].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Negative Sentiment')\n",
    "\n",
    "axs[1].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Positive Sentiment')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
